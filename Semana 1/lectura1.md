# Comment: Collaborative Filtering Recommender Systems

The paper is about, as the title suggests, collaborative filtering (CF): the process of it, uses, how to evaluate it and other related topics. CF is the process of filtering or evaluating items using the opinion of other people. For these recommender systems it is necessary users and items which are connected through different types of ratings or evaluations, where a user rates items. The interactions are used to recommend items, mostly new, to the user. The users may be a group of users and the items a group of items. Furthermore, instead of recommending items, the recommendation could be of other users. Finally, the J.B. Schafer et al. deals with many topics regarding CF, but it does not go into detail.

With the last idea in mind, J.B. Schafer et al. does a really good job introducing CF and giving a lot of information about it. Throughout the paper there are many comparisons and dilemmas regarding CF. For example, collaborative filtering and content-based filtering (CBF), which are not the same, but complement where CF assumes people with similar taste rate things similarly and CBF assumes similar items will be rated similarly. Another comparison is a user-based CF vs. an item-based CF. The user-based uses similar users ratings to predict a rating, whereas item-based uses similar items ratings to predict a rating. In addition, the authors incentivates the readers to continue reading about the topic on other papers and the same book. This is complemented with open questions, challenges for the future and dilemmas such as computing time vs. computing efficiency. All these things are also very well explained using real examples which helps to understand.

A versus I really liked is the explicit vs. implicit rating. The explicit rating is when the user consciously rates an item based on a scale. On the other hand, implicit rating is when the user interacts with an item, but do not actually rate it. The main tradeoff is the ease to obtain the information and the reliability of the information. The explicit rating is more difficult to get, as it is asked to the user for an additional effort, whereas the implicit rating is less reliable as the user did not explicitly give an opinion. These ratings are compared to each other, but it would have been good to complement them. As both have good and bad things, I would think it is better to complement them. This is not suggested by J.B. Schafer et al., but there are successful applications which use both methods. An example is Google Maps. Google Maps uses a 5-star rating (and comments) for the points of interests within the app. Also, it uses the time a user spends on a certain place. For example, when a user spends time eating on a restaurant. Afterwards, Google Maps gives a matching percentage on places with a novelty (non visited by the user) characteristic.

Another topic discussed in the paper is the difficulty to obtain explicit ratings. To deal with this challenge is the idea of giving an incentive to people when they rate. J.B. Schafer et al. gives examples and, although there is no study that evidences the correlation, good reasons to believe this method works. One thing that is not mentioned and I think is important that giving incentives to people will increase the number of false ratings. That is to say some people would start rating just to get the benefits without the rating being real. Consequently, the good idea of the incentives would turn into a bad idea, because the ratings would not be trustworthy. So, to manage the incentives is not easy and there must be a way to track these people who just rates for the benefits.
